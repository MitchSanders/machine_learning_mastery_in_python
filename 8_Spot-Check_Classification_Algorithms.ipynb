{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spot-Check Classification Algorithms\n",
    "\n",
    "><small><i>from the book \n",
    "\"Machine Learning Mastery With Python: Understand Your Data, Create Accurate Models and Work Projects End-To-End\"\n",
    "by Jason Brownlee, Migrated to Jupyter with additions by Mitch Sanders 2017</i></small>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spot-checking is a way of discovering which algorithms perform well on your machine learning\n",
    "problem. You cannot know which algorithms are best suited to your problem beforehand. You\n",
    "must trial a number of methods and focus attention on those that prove themselves the most\n",
    "promising. In this chapter you will discover six machine learning algorithms that you can use\n",
    "when spot-checking your classification problem in Python with scikit-learn. After completing\n",
    "this lesson you will know:\n",
    "\n",
    "1. How to spot-check machine learning algorithms on a classification problem.\n",
    "2. How to spot-check two linear classification algorithms.\n",
    "3. How to spot-check four nonlinear classification algorithms.\n",
    "\n",
    "Let’s get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Spot-Checking\n",
    "\n",
    "You cannot know which algorithm will work best on your dataset beforehand. You must use\n",
    "trial and error to discover a shortlist of algorithms that do well on your problem that you can\n",
    "then double down on and tune further. I call this process spot-checking.\n",
    "The question is not: What algorithm should I use on my dataset? Instead it is: What\n",
    "algorithms should I spot-check on my dataset? You can guess at what algorithms might do\n",
    "well on your dataset, and this can be a good starting point. I recommend trying a mixture of\n",
    "algorithms and see what is good at picking out the structure in your data. Below are some\n",
    "suggestions when spot-checking algorithms on your dataset:\n",
    "\n",
    "- Try a mixture of algorithm representations (e.g. instances and trees).\n",
    "- Try a mixture of learning algorithms (e.g. different algorithms for learning the same type\n",
    "of representation).\n",
    "- Try a mixture of modeling types (e.g. linear and nonlinear functions or parametric and\n",
    "nonparametric).\n",
    "\n",
    "Let’s get specific. In the next section, we will look at algorithms that you can use to\n",
    "spot-check on your next classification machine learning project in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms Overview\n",
    "We are going to take a look at six classification algorithms that you can spot-check on your\n",
    "dataset. Starting with two linear machine learning algorithms:\n",
    "- Logistic Regression.\n",
    "- Linear Discriminant Analysis.\n",
    "\n",
    "Then looking at four nonlinear machine learning algorithms:\n",
    "\n",
    "- k-Nearest Neighbors.\n",
    "- Naive Bayes.\n",
    "- Classification and Regression Trees.\n",
    "- Support Vector Machines.\n",
    "\n",
    "Each recipe is demonstrated on the Pima Indians onset of Diabetes dataset. A test harness\n",
    "using 10-fold cross-validation is used to demonstrate how to spot-check each machine learning\n",
    "algorithm and mean accuracy measures are used to indicate algorithm performance. The recipes\n",
    "assume that you know about each machine learning algorithm and how to use them. We will\n",
    "not go into the API or parameterization of each algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Machine Learning Algorithms\n",
    "This section demonstrates minimal recipes for how to use two linear machine learning algorithms:\n",
    "logistic regression and linear discriminant analysis.\n",
    "\n",
    "### Logistic Regression\n",
    "Logistic regression assumes a Gaussian distribution for the numeric input variables and can\n",
    "model binary classification problems. You can construct a logistic regression model using the\n",
    "LogisticRegression class.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76951469583\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "num_folds = 10\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = LogisticRegression()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())\n",
    "\n",
    "# Running the example prints the mean estimated accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis\n",
    "Linear Discriminant Analysis or LDA is a statistical technique for binary and multiclass\n",
    "classification. It too assumes a Gaussian distribution for the numerical input variables. You can\n",
    "construct an LDA model using the LinearDiscriminantAnalysis class.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.773462064252\n"
     ]
    }
   ],
   "source": [
    "# LDA Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "num_folds = 10\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = LinearDiscriminantAnalysis()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())\n",
    "\n",
    "# Running the example prints the mean estimated accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear Machine Learning Algorithms\n",
    "\n",
    "This section demonstrates minimal recipes for how to use 4 nonlinear machine learning algorithms. \n",
    "\n",
    "### k-Nearest Neighbors\n",
    "The k-Nearest Neighbors algorithm (or KNN) uses a distance metric to find the k most similar\n",
    "instances in the training data for a new instance and takes the mean outcome of the neighbors\n",
    "as the prediction. You can construct a KNN model using the KNeighborsClassifier class.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.726555023923\n"
     ]
    }
   ],
   "source": [
    "# KNN Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "num_folds = 10\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = KNeighborsClassifier()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())\n",
    "\n",
    "# Running the example prints the mean estimated accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "Naive Bayes calculates the probability of each class and the conditional probability of each class\n",
    "given each input value. These probabilities are estimated for new data and multiplied together,\n",
    "assuming that they are all independent (a simple or naive assumption). When working with\n",
    "real-valued data, a Gaussian distribution is assumed to easily estimate the probabilities for\n",
    "input variables using the Gaussian Probability Density Function. You can construct a Naive\n",
    "Bayes model using the GaussianNB class.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75517771702\n"
     ]
    }
   ],
   "source": [
    "# Gaussian Naive Bayes Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = GaussianNB()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())\n",
    "\n",
    "# Running the example prints the mean estimated accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification and Regression Trees\n",
    "\n",
    "Classification and Regression Trees (CART or just decision trees) construct a binary tree from\n",
    "the training data. Split points are chosen greedily by evaluating each attribute and each value\n",
    "of each attribute in the training data in order to minimize a cost function (like the Gini index).\n",
    "You can construct a CART model using the DecisionTreeClassifier class.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.692634996582\n"
     ]
    }
   ],
   "source": [
    "# CART Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = DecisionTreeClassifier()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())\n",
    "\n",
    "# Running the example prints the mean estimated accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "\n",
    "Support Vector Machines (or SVM) seek a line that best separates two classes. Those data\n",
    "instances that are closest to the line that best separates the classes are called support vectors\n",
    "and influence where the line is placed. SVM has been extended to support multiple classes.\n",
    "Of particular importance is the use of different kernel functions via the kernel parameter. A powerful Radial Basis Function is used by default. You can construct an SVM model using the\n",
    "SVC class.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.651025290499\n"
     ]
    }
   ],
   "source": [
    "# SVM Classification\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "kfold = KFold(n_splits=10, random_state=7)\n",
    "model = SVC()\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())\n",
    "\n",
    "# Running the example prints the mean estimated accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this chapter you discovered 6 machine learning algorithms that you can use to spot-check\n",
    "on your classification problem in Python using scikit-learn. Specifically, you learned how to\n",
    "spot-check two linear machine learning algorithms: Logistic Regression and Linear Discriminant\n",
    "Analysis. You also learned how to spot-check four nonlinear algorithms: k-Nearest Neighbors,\n",
    "Naive Bayes, Classification and Regression Trees and Support Vector Machines.\n",
    "\n",
    "### Next\n",
    "In the next lesson you will discover how you can use spot-checking on regression machine learning\n",
    "problems and practice with seven different regression algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Boston House Price dataset:\n",
    "Maintained at UCI machine Learning Repository\n",
    "https://archive.ics.uci.edu/ml/datasets/housing\n",
    "\n",
    "Included in scikit-learn datasets module\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html\n",
    "\n",
    "\n",
    "#### Attribute Information:\n",
    "\n",
    "1. CRIM: per capita crime rate by town \n",
    "2. ZN: proportion of residential land zoned for lots over 25,000 sq.ft. \n",
    "3. INDUS: proportion of non-retail business acres per town \n",
    "4. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) \n",
    "5. NOX: nitric oxides concentration (parts per 10 million) \n",
    "6. RM: average number of rooms per dwelling \n",
    "7. AGE: proportion of owner-occupied units built prior to 1940 \n",
    "8. DIS: weighted distances to five Boston employment centres \n",
    "9. RAD: index of accessibility to radial highways \n",
    "10. TAX: full-value property-tax rate per 10,000 US Dollars\n",
    "11. PTRATIO: pupil-teacher ratio by town \n",
    "12. B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town \n",
    "13. LSTAT: % lower status of the population \n",
    "14. MEDV: Median value of owner-occupied homes in 1000's US Dollars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# demo using Boston Housing data in SciKit-learn\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "print(boston.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Pima Indian Dataset \n",
    "\n",
    "#### Attribute Information:\n",
    "\n",
    "1. Number of times pregnant \n",
    "2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test \n",
    "3. Diastolic blood pressure (mm Hg) \n",
    "4. Triceps skin fold thickness (mm) \n",
    "5. 2-Hour serum insulin (mu U/ml) \n",
    "6. Body mass index (weight in kg/(height in m)^2) \n",
    "7. Diabetes pedigree function \n",
    "8. Age (years) \n",
    "9. Class variable (0 or 1) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
